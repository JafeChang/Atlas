#!/usr/bin/env python3
"""
Atlas æ•°æ®åº“æŸ¥çœ‹å·¥å…·

ç®€å•ç›´æ¥çš„æ•°æ®åº“æŸ¥è¯¢å’ŒæŸ¥çœ‹åŠŸèƒ½ã€‚
"""

import sqlite3
import json
import argparse
from datetime import datetime
from pathlib import Path


def connect_db(db_path="data/atlas.db"):
    """è¿æ¥æ•°æ®åº“"""
    if not Path(db_path).exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return None
    return sqlite3.connect(db_path)


def show_tables(conn):
    """æ˜¾ç¤ºæ‰€æœ‰è¡¨"""
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = cursor.fetchall()

    print("ğŸ“Š æ•°æ®åº“è¡¨:")
    for i, (table_name,) in enumerate(tables, 1):
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        print(f"  {i}. {table_name}: {count:,} æ¡è®°å½•")


def show_table_structure(conn, table_name):
    """æ˜¾ç¤ºè¡¨ç»“æ„"""
    cursor = conn.cursor()
    cursor.execute(f"PRAGMA table_info({table_name})")
    columns = cursor.fetchall()

    print(f"\nğŸ“‹ è¡¨ '{table_name}' ç»“æ„:")
    print(f"{'åˆ—å':<20} {'ç±»å‹':<15} {'æ˜¯å¦ä¸ºç©º':<8} {'é»˜è®¤å€¼':<15} {'ä¸»é”®':<5}")
    print("-" * 80)

    for col in columns:
        cid, name, col_type, not_null, default_val, pk = col
        print(f"{name:<20} {col_type:<15} {'NO' if not_null else 'YES':<8} {str(default_val or ''):<15} {'PK' if pk else ''}")


def show_table_data(conn, table_name, limit=10):
    """æ˜¾ç¤ºè¡¨æ•°æ®"""
    cursor = conn.cursor()
    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    total_count = cursor.fetchone()[0]

    if total_count == 0:
        print(f"ğŸ“ è¡¨ '{table_name}' æ²¡æœ‰æ•°æ®")
        return

    print(f"\nğŸ“„ è¡¨ '{table_name}' æ•°æ® (æ€»å…± {total_count:,} æ¡, æ˜¾ç¤ºå‰ {limit} æ¡):")

    cursor.execute(f"SELECT * FROM {table_name} LIMIT {limit}")
    rows = cursor.fetchall()

    # è·å–åˆ—å
    cursor.execute(f"PRAGMA table_info({table_name})")
    columns = [col[1] for col in cursor.fetchall()]

    # æ˜¾ç¤ºè¡¨å¤´
    print(f"{'#':<4} | {' | '.join(col[:12] for col in columns)}")
    print("-" * (4 + len(columns) * 15))

    # æ˜¾ç¤ºæ•°æ®
    for i, row in enumerate(rows, 1):
        row_str = " | ".join(str(cell)[:12] if cell is not None else 'NULL' for cell in row)
        print(f"{i:<4} | {row_str}")


def show_sources_summary(conn):
    """æ˜¾ç¤ºæ•°æ®æºæ±‡æ€»"""
    cursor = conn.cursor()

    cursor.execute('''
        SELECT
            COUNT(*) as total,
            SUM(CASE WHEN enabled = 1 THEN 1 ELSE 0 END) as enabled,
            SUM(CASE WHEN enabled = 0 THEN 1 ELSE 0 END) as disabled,
            SUM(collection_count) as total_collections,
            SUM(success_count) as total_successes,
            SUM(error_count) as total_errors
        FROM data_sources
    ''')

    stats = cursor.fetchone()
    total, enabled, disabled, total_coll, total_success, total_errors = stats

    print("\nğŸ“¡ æ•°æ®æºæ±‡æ€»:")
    print(f"  æ€»æ•°: {total}")
    print(f"  å¯ç”¨: {enabled}")
    print(f"  ç¦ç”¨: {disabled}")
    print(f"  æ€»é‡‡é›†æ¬¡æ•°: {total_coll or 0}")
    print(f"  æ€»æˆåŠŸæ¬¡æ•°: {total_success or 0}")
    print(f"  æ€»å¤±è´¥æ¬¡æ•°: {total_errors or 0}")

    if total_coll and total_coll > 0:
        success_rate = (total_success / total_coll) * 100
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")


def query_data_sources(conn, enabled_only=True):
    """æŸ¥è¯¢æ•°æ®æº"""
    cursor = conn.cursor()

    where_clause = "WHERE enabled = 1" if enabled_only else ""

    cursor.execute(f'''
        SELECT name, source_type, url, collection_count,
               success_count, error_count, last_success_at
        FROM data_sources
        {where_clause}
        ORDER BY name
    ''')

    sources = cursor.fetchall()

    print(f"\nğŸ“Š {'å¯ç”¨' if enabled_only else 'æ‰€æœ‰'}æ•°æ®æº:")
    print(f"{'åç§°':<25} {'ç±»å‹':<10} {'é‡‡é›†æ¬¡æ•°':<8} {'æˆåŠŸæ¬¡æ•°':<8} {'å¤±è´¥æ¬¡æ•°':<8} {'æœ€åæˆåŠŸ'}")
    print("-" * 80)

    for source in sources:
        name, stype, url, coll_count, succ_count, err_count, last_success = source

        last_success_str = "N/A"
        if last_success:
            try:
                dt = datetime.fromisoformat(last_success.replace('Z', '+00:00'))
                last_success_str = dt.strftime("%m-%d %H:%M")
            except:
                last_success_str = "Invalid"

        print(f"{name:<25} {stype:<10} {coll_count or 0:<8} {succ_count or 0:<8} {err_count or 0:<8} {last_success_str}")


def export_to_json(conn, output_file="atlas_audit.json"):
    """å¯¼å‡ºå®¡è®¡æ•°æ®åˆ°JSON"""
    cursor = conn.cursor()

    audit_data = {
        "export_time": datetime.now().isoformat(),
        "tables": {},
        "data_sources": []
    }

    # å¯¼å‡ºæ‰€æœ‰è¡¨çš„ç»Ÿè®¡ä¿¡æ¯
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = [row[0] for row in cursor.fetchall()]

    for table in tables:
        cursor.execute(f"SELECT COUNT(*) FROM {table}")
        count = cursor.fetchone()[0]
        audit_data["tables"][table] = count

    # å¯¼å‡ºæ•°æ®æºè¯¦æƒ…
    cursor.execute('''
        SELECT name, source_type, url, enabled, collection_interval,
               collection_count, success_count, error_count, last_success_at,
               created_at, updated_at
        FROM data_sources
        ORDER BY name
    ''')

    for row in cursor.fetchall():
        audit_data["data_sources"].append({
            "name": row[0],
            "type": row[1],
            "url": row[2],
            "enabled": bool(row[3]),
            "interval": row[4],
            "collections": row[5] or 0,
            "successes": row[6] or 0,
            "errors": row[7] or 0,
            "last_success": row[8],
            "created_at": row[9],
            "updated_at": row[10]
        })

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(audit_data, f, indent=2, ensure_ascii=False)

    print(f"âœ… å®¡è®¡æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Atlas æ•°æ®åº“æŸ¥çœ‹å·¥å…·")
    parser.add_argument("--db", default="data/atlas.db", help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--tables", action="store_true", help="æ˜¾ç¤ºæ‰€æœ‰è¡¨")
    parser.add_argument("--structure", metavar="TABLE", help="æ˜¾ç¤ºæŒ‡å®šè¡¨çš„ç»“æ„")
    parser.add_argument("--data", metavar="TABLE", help="æ˜¾ç¤ºæŒ‡å®šè¡¨çš„æ•°æ®")
    parser.add_argument("--limit", type=int, default=10, help="æ•°æ®æ˜¾ç¤ºæ¡æ•°é™åˆ¶")
    parser.add_argument("--sources", action="store_true", help="æ˜¾ç¤ºæ•°æ®æºä¿¡æ¯")
    parser.add_argument("--all-sources", action="store_true", help="æ˜¾ç¤ºæ‰€æœ‰æ•°æ®æºï¼ˆåŒ…æ‹¬ç¦ç”¨çš„ï¼‰")
    parser.add_argument("--export", metavar="FILE", help="å¯¼å‡ºå®¡è®¡æ•°æ®åˆ°JSONæ–‡ä»¶")
    parser.add_argument("--summary", action="store_true", help="æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯")

    args = parser.parse_args()

    conn = connect_db(args.db)
    if not conn:
        return

    try:
        if args.tables:
            show_tables(conn)

        elif args.structure:
            show_table_structure(conn, args.structure)

        elif args.data:
            show_table_data(conn, args.data, args.limit)

        elif args.sources:
            query_data_sources(conn, enabled_only=True)

        elif args.all_sources:
            query_data_sources(conn, enabled_only=False)

        elif args.export:
            export_to_json(conn, args.export)

        elif args.summary:
            show_tables(conn)
            show_sources_summary(conn)

        else:
            # é»˜è®¤æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            print("ğŸ—„ï¸ Atlas æ•°æ®åº“æŸ¥çœ‹å·¥å…·")
            print(f"ğŸ“ æ•°æ®åº“: {args.db}")

            db_file = Path(args.db)
            if db_file.exists():
                size_mb = db_file.stat().st_size / (1024 * 1024)
                print(f"ğŸ“ å¤§å°: {size_mb:.1f} MB")

            show_tables(conn)
            show_sources_summary(conn)

            print(f"\nğŸ’¡ ä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šé€‰é¡¹")

    finally:
        conn.close()


if __name__ == "__main__":
    main()