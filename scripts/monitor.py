#!/usr/bin/env python3
"""
Atlas ç³»ç»Ÿç›‘æ§å·¥å…·

æä¾›å®æ—¶ç›‘æ§ã€çŠ¶æ€æŸ¥çœ‹ã€æ•°æ®åˆ†æç­‰åŠŸèƒ½ã€‚
"""

import time
import sqlite3
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import threading
import sys


class AtlasMonitor:
    """Atlas ç³»ç»Ÿç›‘æ§å™¨"""

    def __init__(self, db_path: str = "data/atlas.db", log_path: str = "logs"):
        self.db_path = db_path
        self.log_path = Path(log_path)
        self.running = False

    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        if not Path(self.db_path).exists():
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.db_path}")
            return None
        return sqlite3.connect(self.db_path)

    def get_system_overview(self) -> Dict:
        """è·å–ç³»ç»Ÿæ¦‚è§ˆ"""
        conn = self.connect_db()
        if not conn:
            return {}

        cursor = conn.cursor()

        overview = {
            "timestamp": datetime.now().isoformat(),
            "database_size": 0,
            "data_sources": {"total": 0, "enabled": 0, "disabled": 0},
            "collections": {"total": 0, "success": 0, "failed": 0},
            "documents": {"raw": 0, "processed": 0, "today": 0},
            "storage": {"files_count": 0, "total_size": 0}
        }

        try:
            # æ•°æ®åº“æ–‡ä»¶å¤§å°
            db_file = Path(self.db_path)
            if db_file.exists():
                overview["database_size"] = db_file.stat().st_size

            # æ•°æ®æºç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM data_sources")
            overview["data_sources"]["total"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM data_sources WHERE enabled = 1")
            overview["data_sources"]["enabled"] = cursor.fetchone()[0]

            overview["data_sources"]["disabled"] = (
                overview["data_sources"]["total"] - overview["data_sources"]["enabled"]
            )

            # é‡‡é›†ç»Ÿè®¡
            cursor.execute("SELECT SUM(collection_count), SUM(success_count), SUM(error_count) FROM data_sources")
            result = cursor.fetchone()
            overview["collections"]["total"] = result[0] or 0
            overview["collections"]["success"] = result[1] or 0
            overview["collections"]["failed"] = result[2] or 0

            # æ–‡æ¡£ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) FROM raw_documents")
            overview["documents"]["raw"] = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM processed_documents")
            overview["documents"]["processed"] = cursor.fetchone()[0]

            # ä»Šæ—¥é‡‡é›†
            today = datetime.now().date().isoformat()
            cursor.execute(
                "SELECT COUNT(*) FROM raw_documents WHERE DATE(collected_at) = ?",
                (today,)
            )
            overview["documents"]["today"] = cursor.fetchone()[0]

            # æ–‡ä»¶å­˜å‚¨ç»Ÿè®¡
            data_dir = Path("data/raw")
            if data_dir.exists():
                json_files = list(data_dir.rglob("*.json"))
                overview["storage"]["files_count"] = len(json_files)

                total_size = sum(f.stat().st_size for f in json_files)
                overview["storage"]["total_size"] = total_size

        except Exception as e:
            print(f"âŒ è·å–ç³»ç»Ÿæ¦‚è§ˆå¤±è´¥: {e}")

        finally:
            conn.close()

        return overview

    def format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"

    def format_timestamp(self, timestamp_str: Optional[str]) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        if not timestamp_str:
            return "N/A"
        try:
            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            return dt.strftime("%m-%d %H:%M")
        except:
            return timestamp_str

    def display_overview(self):
        """æ˜¾ç¤ºç³»ç»Ÿæ¦‚è§ˆ"""
        overview = self.get_system_overview()

        print("ğŸ—„ï¸  Atlas ç³»ç»Ÿæ¦‚è§ˆ")
        print("=" * 50)
        print(f"ğŸ“Š æŸ¥è¯¢æ—¶é—´: {overview['timestamp']}")
        print(f"ğŸ’¾ æ•°æ®åº“å¤§å°: {self.format_size(overview['database_size'])}")
        print(f"ğŸ“ å­˜å‚¨æ–‡ä»¶: {overview['storage']['files_count']} ä¸ª ({self.format_size(overview['storage']['total_size'])})")
        print()

        print("ğŸ“¡ æ•°æ®æºçŠ¶æ€:")
        print(f"  æ€»æ•°: {overview['data_sources']['total']}")
        print(f"  âœ… å¯ç”¨: {overview['data_sources']['enabled']}")
        print(f"  âŒ ç¦ç”¨: {overview['data_sources']['disabled']}")
        print()

        print("ğŸ”„ é‡‡é›†ç»Ÿè®¡:")
        print(f"  æ€»é‡‡é›†æ¬¡æ•°: {overview['collections']['total']}")
        print(f"  âœ… æˆåŠŸæ¬¡æ•°: {overview['collections']['success']}")
        print(f"  âŒ å¤±è´¥æ¬¡æ•°: {overview['collections']['failed']}")

        if overview['collections']['total'] > 0:
            success_rate = (overview['collections']['success'] / overview['collections']['total']) * 100
            print(f"  ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print()

        print("ğŸ“„ æ–‡æ¡£ç»Ÿè®¡:")
        print(f"  åŸå§‹æ–‡æ¡£: {overview['documents']['raw']}")
        print(f"  å¤„ç†æ–‡æ¡£: {overview['documents']['processed']}")
        print(f"  ğŸ—“ï¸  ä»Šæ—¥æ–°å¢: {overview['documents']['today']}")

    def show_data_sources_status(self):
        """æ˜¾ç¤ºæ•°æ®æºçŠ¶æ€"""
        conn = self.connect_db()
        if not conn:
            return

        cursor = conn.cursor()

        cursor.execute('''
            SELECT name, source_type, enabled, collection_count,
                   success_count, error_count, last_success_at, last_error
            FROM data_sources
            ORDER BY enabled DESC, name
        ''')

        sources = cursor.fetchall()

        print("\nğŸ“¡ æ•°æ®æºè¯¦ç»†çŠ¶æ€:")
        print("=" * 80)
        print(f"{'åç§°':<25} {'ç±»å‹':<10} {'çŠ¶æ€':<8} {'é‡‡é›†/æˆåŠŸ/å¤±è´¥':<15} {'æœ€åæˆåŠŸ':<12} {'çŠ¶æ€æè¿°'}")
        print("-" * 80)

        for source in sources:
            name, stype, enabled, coll_count, succ_count, err_count, last_success, last_error = source

            status = "âœ… å¯ç”¨" if enabled else "âŒ ç¦ç”¨"
            coll_str = f"{coll_count or 0}/{succ_count or 0}/{err_count or 0}"
            last_success_str = self.format_timestamp(last_success)

            # çŠ¶æ€æè¿°
            if enabled and coll_count > 0:
                if err_count > 0:
                    status_desc = f"âš ï¸  {last_error[:30] if last_error else 'æœ‰é”™è¯¯'}"
                else:
                    status_desc = "âœ… æ­£å¸¸"
            elif enabled and coll_count == 0:
                status_desc = "â³ å¾…é‡‡é›†"
            else:
                status_desc = "âŒ å·²ç¦ç”¨"

            print(f"{name:<25} {stype:<10} {status:<8} {coll_str:<15} {last_success_str:<12} {status_desc}")

        conn.close()

    def show_recent_collections(self, limit: int = 10):
        """æ˜¾ç¤ºæœ€è¿‘çš„é‡‡é›†æ´»åŠ¨"""
        conn = self.connect_db()
        if not conn:
            return

        cursor = conn.cursor()

        cursor.execute('''
            SELECT source_id, created_at, started_at, completed_at,
                   status, items_collected, items_processed, items_failed,
                   error_message
            FROM collection_tasks
            ORDER BY created_at DESC
            LIMIT ?
        ''', (limit,))

        tasks = cursor.fetchall()

        if not tasks:
            print("\nğŸ“‹ æš‚æ— é‡‡é›†ä»»åŠ¡è®°å½•")
            conn.close()
            return

        print(f"\nğŸ“‹ æœ€è¿‘é‡‡é›†ä»»åŠ¡ (æœ€è¿‘{limit}ä¸ª):")
        print("=" * 90)
        print(f"{'æ•°æ®æº':<20} {'å¼€å§‹æ—¶é—´':<12} {'çŠ¶æ€':<8} {'é‡‡é›†/å¤„ç†/å¤±è´¥':<15} {'è€—æ—¶':<8} {'é”™è¯¯ä¿¡æ¯'}")
        print("-" * 90)

        for task in tasks:
            (source_id, created_at, started_at, completed_at, status,
             items_collected, items_processed, items_failed, error_message) = task

            start_time = self.format_timestamp(started_at)
            status_icon = {"completed": "âœ…", "running": "ğŸ”„", "failed": "âŒ", "pending": "â³"}.get(status, "â“")

            items_str = f"{items_collected or 0}/{items_processed or 0}/{items_failed or 0}"

            # è®¡ç®—è€—æ—¶
            if started_at and completed_at:
                try:
                    start_dt = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
                    end_dt = datetime.fromisoformat(completed_at.replace('Z', '+00:00'))
                    duration = (end_dt - start_dt).total_seconds()
                    duration_str = f"{duration:.0f}s"
                except:
                    duration_str = "N/A"
            else:
                duration_str = "N/A"

            error_str = (error_message or "")[:20] + "..." if error_message and len(error_message) > 20 else (error_message or "")

            print(f"{source_id:<20} {start_time:<12} {status_icon+status:<8} {items_str:<15} {duration_str:<8} {error_str}")

        conn.close()

    def show_collected_data(self, limit: int = 20):
        """æ˜¾ç¤ºé‡‡é›†åˆ°çš„æ•°æ®"""
        # æ£€æŸ¥JSONæ–‡ä»¶
        data_dir = Path("data/raw")
        if not data_dir.exists():
            print("\nğŸ“ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return

        json_files = list(data_dir.rglob("summary_*.json"))

        if not json_files:
            print("\nğŸ“„ æš‚æ— é‡‡é›†æ•°æ®")
            return

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        json_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)

        print(f"\nğŸ“„ é‡‡é›†æ•°æ®æ¦‚è§ˆ (æœ€æ–°{min(limit, len(json_files))}ä¸ªæ–‡ä»¶):")
        print("=" * 80)

        for i, json_file in enumerate(json_files[:limit]):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                file_time = datetime.fromtimestamp(json_file.stat().st_mtime)
                source_name = data.get('source_name', 'Unknown')
                items_count = data.get('items_count', 0)
                collected_at = data.get('collected_at', 'Unknown')

                print(f"{i+1:2d}. ğŸ“ {source_name:<20} ({json_file.parent.name})")
                print(f"     ğŸ“Š é¡¹ç›®æ•°: {items_count}")
                print(f"     ğŸ•’ é‡‡é›†æ—¶é—´: {collected_at[:19]}")
                print(f"     ğŸ“‚ æ–‡ä»¶ä½ç½®: {json_file.relative_to(Path.cwd())}")

                # æ˜¾ç¤ºæœ€æ–°çš„å‡ ä¸ªé¡¹ç›®æ ‡é¢˜
                if data.get('items') and len(data['items']) > 0:
                    print(f"     ğŸ“‹ æœ€æ–°é¡¹ç›®:")
                    for item in data['items'][:3]:
                        title = item.get('title', 'No Title')
                        # æˆªæ–­é•¿æ ‡é¢˜
                        if len(title) > 50:
                            title = title[:47] + "..."
                        print(f"        â€¢ {title}")

                print()

            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {json_file}: {e}")

    def show_log_tail(self, lines: int = 20):
        """æ˜¾ç¤ºæ—¥å¿—å°¾éƒ¨"""
        log_file = self.log_path / "atlas.log"

        if not log_file.exists():
            print("\nğŸ“ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
            return

        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            print(f"\nğŸ“ æœ€è¿‘æ—¥å¿— (æœ€æ–°{lines}è¡Œ):")
            print("=" * 60)

            for line in all_lines[-lines:]:
                print(line.rstrip())

        except Exception as e:
            print(f"âŒ è¯»å–æ—¥å¿—å¤±è´¥: {e}")

    def monitor_loop(self, interval: int = 30):
        """å®æ—¶ç›‘æ§å¾ªç¯"""
        self.running = True

        try:
            while self.running:
                # æ¸…å±
                import os
                os.system('clear' if os.name == 'posix' else 'cls')

                print("ğŸ—„ï¸  Atlas å®æ—¶ç›‘æ§")
                print("=" * 50)
                print(f"â° ç›‘æ§æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"ğŸ”„ åˆ·æ–°é—´éš”: {interval}ç§’")
                print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
                print()

                self.display_overview()
                self.show_data_sources_status()

                # ç­‰å¾…ä¸‹æ¬¡åˆ·æ–°
                time.sleep(interval)

        except KeyboardInterrupt:
            print("\n\nğŸ›‘ ç›‘æ§å·²åœæ­¢")
        finally:
            self.running = False

    def export_dashboard_data(self, output_file: str = "dashboard.json"):
        """å¯¼å‡ºä»ªè¡¨æ¿æ•°æ®"""
        overview = self.get_system_overview()

        # è·å–è¯¦ç»†çš„æ•°æ®æºä¿¡æ¯
        conn = self.connect_db()
        sources_data = []

        if conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT name, source_type, enabled, collection_count,
                       success_count, error_count, last_success_at, url
                FROM data_sources
                ORDER BY name
            ''')

            for row in cursor.fetchall():
                sources_data.append({
                    "name": row[0],
                    "type": row[1],
                    "enabled": bool(row[2]),
                    "collections": row[3] or 0,
                    "successes": row[4] or 0,
                    "errors": row[5] or 0,
                    "last_success": row[6],
                    "url": row[7]
                })

            conn.close()

        dashboard_data = {
            "overview": overview,
            "sources": sources_data,
            "export_time": datetime.now().isoformat()
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dashboard_data, f, indent=2, ensure_ascii=False)

        print(f"âœ… ä»ªè¡¨æ¿æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Atlas ç³»ç»Ÿç›‘æ§å·¥å…·")
    parser.add_argument("--db", default="data/atlas.db", help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--log", default="logs", help="æ—¥å¿—æ–‡ä»¶è·¯å¾„")

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # æ¦‚è§ˆå‘½ä»¤
    subparsers.add_parser('overview', help='æ˜¾ç¤ºç³»ç»Ÿæ¦‚è§ˆ')

    # æ•°æ®æºçŠ¶æ€
    subparsers.add_parser('sources', help='æ˜¾ç¤ºæ•°æ®æºçŠ¶æ€')

    # é‡‡é›†å†å²
    subparsers.add_parser('history', help='æ˜¾ç¤ºé‡‡é›†å†å²')

    # æ•°æ®æŸ¥çœ‹
    parser_data = subparsers.add_parser('data', help='æŸ¥çœ‹é‡‡é›†æ•°æ®')
    parser_data.add_argument('--limit', type=int, default=20, help='æ˜¾ç¤ºæ¡ç›®é™åˆ¶')

    # æ—¥å¿—æŸ¥çœ‹
    parser_log = subparsers.add_parser('logs', help='æŸ¥çœ‹æ—¥å¿—')
    parser_log.add_argument('--lines', type=int, default=20, help='æ˜¾ç¤ºè¡Œæ•°')

    # å®æ—¶ç›‘æ§
    parser_monitor = subparsers.add_parser('monitor', help='å®æ—¶ç›‘æ§')
    parser_monitor.add_argument('--interval', type=int, default=30, help='åˆ·æ–°é—´éš”(ç§’)')

    # å¯¼å‡ºæ•°æ®
    parser_export = subparsers.add_parser('export', help='å¯¼å‡ºä»ªè¡¨æ¿æ•°æ®')
    parser_export.add_argument('--output', default='dashboard.json', help='è¾“å‡ºæ–‡ä»¶')

    args = parser.parse_args()

    monitor = AtlasMonitor(args.db, args.log)

    if args.command == 'overview':
        monitor.display_overview()
        monitor.show_data_sources_status()

    elif args.command == 'sources':
        monitor.show_data_sources_status()

    elif args.command == 'history':
        monitor.show_recent_collections()

    elif args.command == 'data':
        monitor.show_collected_data(args.limit)

    elif args.command == 'logs':
        monitor.show_log_tail(args.lines)

    elif args.command == 'monitor':
        print("ğŸš€ å¯åŠ¨å®æ—¶ç›‘æ§...")
        monitor.monitor_loop(args.interval)

    elif args.command == 'export':
        monitor.export_dashboard_data(args.output)

    else:
        # é»˜è®¤æ˜¾ç¤ºæ¦‚è§ˆ
        monitor.display_overview()
        monitor.show_data_sources_status()
        print(f"\nğŸ’¡ ä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šé€‰é¡¹")


if __name__ == "__main__":
    main()