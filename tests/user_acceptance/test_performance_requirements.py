"""
æ€§èƒ½è¦æ±‚ç”¨æˆ·éªŒæ”¶æµ‹è¯•

éªŒè¯Atlasç³»ç»Ÿçš„æ€§èƒ½æ˜¯å¦æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚
"""

import pytest
import asyncio
import tempfile
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

from atlas.core.config import get_config
from atlas.collectors.rss_collector import RSSCollector
from atlas.processors.parser import HTMLParser
from atlas.processors.normalizer import TextNormalizer


class TestPerformanceRequirements:
    """æ€§èƒ½è¦æ±‚éªŒæ”¶æµ‹è¯•"""

    @pytest.fixture
    def test_environment(self):
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•ç¯å¢ƒ"""
        with tempfile.TemporaryDirectory() as temp_dir:
            config_dir = Path(temp_dir) / "config"
            data_dir = Path(temp_dir) / "data"
            log_dir = Path(temp_dir) / "logs"

            for directory in [config_dir, data_dir, log_dir]:
                directory.mkdir(parents=True, exist_ok=True)

            config = get_config(config_dir=config_dir)
            config._config_data["data_dir"] = str(data_dir)
            config._config_data["log_dir"] = str(log_dir)

            yield {
                "config": config,
                "data_dir": data_dir
            }

    def test_perf_01_rss_collection_speed(self, test_environment):
        """æ€§èƒ½æµ‹è¯•1: RSSé‡‡é›†é€Ÿåº¦è¦æ±‚"""
        print("ğŸš€ æ€§èƒ½æµ‹è¯• 1: RSSé‡‡é›†é€Ÿåº¦")

        rss_collector = RSSCollector(
            user_agent="Atlas/1.0 (Performance Test)",
            timeout=30,
            max_concurrent=5
        )

        # ä½¿ç”¨å¤šä¸ªRSSæºè¿›è¡Œå¹¶å‘æµ‹è¯•
        rss_urls = [
            "https://feeds.bbci.co.uk/news/rss.xml",
            "https://rss.cnn.com/rss/edition.rss",
            "https://feeds.reuters.com/reuters/topNews"
        ]

        async def concurrent_collection():
            start_time = time.time()

            tasks = [rss_collector.collect_rss(url) for url in rss_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            end_time = time.time()
            total_time = end_time - start_time

            successful_results = [r for r in results if not isinstance(r, Exception)]
            total_items = sum(len(result.items) if hasattr(result, 'items') else 0
                             for result in successful_results)

            # æ€§èƒ½è¦æ±‚: æ¯ä¸ªRSSæºé‡‡é›†æ—¶é—´ä¸åº”è¶…è¿‡30ç§’
            avg_time_per_source = total_time / len(rss_urls)
            assert avg_time_per_source <= 30.0, f"å¹³å‡é‡‡é›†æ—¶é—´è¿‡é•¿: {avg_time_per_source:.2f}ç§’"

            # æ€§èƒ½è¦æ±‚: æ€»é‡‡é›†æ—¶é—´ä¸åº”è¶…è¿‡60ç§’
            assert total_time <= 60.0, f"æ€»é‡‡é›†æ—¶é—´è¿‡é•¿: {total_time:.2f}ç§’"

            # æ€§èƒ½è¦æ±‚: åº”è¯¥é‡‡é›†åˆ°å†…å®¹
            assert total_items > 0, "åº”è¯¥é‡‡é›†åˆ°RSSå†…å®¹"
            assert len(successful_results) >= len(rss_urls) * 0.7, "æˆåŠŸç‡åº”è¯¥ä¸ä½äº70%"

            print(f"âœ… é‡‡é›†æ€§èƒ½è¾¾æ ‡:")
            print(f"   - æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   - å¹³å‡æ¯æº: {avg_time_per_source:.2f}ç§’")
            print(f"   - æˆåŠŸç‡: {len(successful_results)}/{len(rss_urls)}")
            print(f"   - æ€»æ¡ç›®: {total_items}")

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(concurrent_collection())
        finally:
            loop.close()

    def test_perf_02_html_processing_speed(self, test_environment):
        """æ€§èƒ½æµ‹è¯•2: HTMLå¤„ç†é€Ÿåº¦è¦æ±‚"""
        print("ğŸš€ æ€§èƒ½æµ‹è¯• 2: HTMLå¤„ç†é€Ÿåº¦")

        html_parser = HTMLParser()
        text_normalizer = TextNormalizer()

        # ç”Ÿæˆå¤§é‡HTMLå†…å®¹è¿›è¡Œæ€§èƒ½æµ‹è¯•
        test_html = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>æ€§èƒ½æµ‹è¯•é¡µé¢</title>
            <meta name="description" content="è¿™æ˜¯ä¸€ä¸ªç”¨äºæ€§èƒ½æµ‹è¯•çš„é¡µé¢">
        </head>
        <body>
            <h1>ä¸»æ ‡é¢˜</h1>
            <div class="article">
                <p>æ®µè½1: Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
                <p>æ®µè½2: Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
                <p>æ®µè½3: Ut enim ad minim veniam, quis nostrud exercitation ullamco.</p>
            </div>
            <div class="sidebar">
                <h2>ä¾§è¾¹æ </h2>
                <p>ä¾§è¾¹æ å†…å®¹</p>
            </div>
        </body>
        </html>
        """

        # æ€§èƒ½è¦æ±‚: å•ä¸ªHTMLæ–‡æ¡£å¤„ç†æ—¶é—´ä¸åº”è¶…è¿‡1ç§’
        num_documents = 50
        start_time = time.time()

        for i in range(num_documents):
            try:
                parsed = html_parser.parse_html_content(
                    test_html,
                    url=f"https://example.com/test-{i}",
                    title_selector="h1",
                    content_selector=".article p"
                )

                normalized = text_normalizer.normalize_text(parsed.content)

                # éªŒè¯å¤„ç†ç»“æœ
                assert parsed.title == "ä¸»æ ‡é¢˜", f"æ–‡æ¡£{i}: æ ‡é¢˜è§£æé”™è¯¯"
                assert len(normalized.strip()) > 0, f"æ–‡æ¡£{i}: æ ‡å‡†åŒ–åå†…å®¹ä¸ºç©º"

            except Exception as e:
                pytest.fail(f"æ–‡æ¡£{i}å¤„ç†å¤±è´¥: {e}")

        end_time = time.time()
        total_time = end_time - start_time
        avg_time_per_doc = total_time / num_documents

        # æ€§èƒ½éªŒè¯
        assert avg_time_per_doc <= 1.0, f"å¹³å‡å¤„ç†æ—¶é—´è¿‡é•¿: {avg_time_per_doc:.3f}ç§’/æ–‡æ¡£"
        assert total_time <= 30.0, f"æ€»å¤„ç†æ—¶é—´è¿‡é•¿: {total_time:.2f}ç§’"

        print(f"âœ… HTMLå¤„ç†æ€§èƒ½è¾¾æ ‡:")
        print(f"   - æ–‡æ¡£æ•°é‡: {num_documents}")
        print(f"   - æ€»æ—¶é—´: {total_time:.2f}ç§’")
        print(f"   - å¹³å‡æ¯æ–‡æ¡£: {avg_time_per_doc:.3f}ç§’")
        print(f"   - å¤„ç†é€Ÿåº¦: {num_documents/total_time:.1f} æ–‡æ¡£/ç§’")

    def test_perf_03_memory_usage(self, test_environment):
        """æ€§èƒ½æµ‹è¯•3: å†…å­˜ä½¿ç”¨è¦æ±‚"""
        print("ğŸš€ æ€§èƒ½æµ‹è¯• 3: å†…å­˜ä½¿ç”¨")

        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # åˆ›å»ºå¤šä¸ªç»„ä»¶å®ä¾‹æµ‹è¯•å†…å­˜ä½¿ç”¨
        components = []
        for i in range(20):
            rss_collector = RSSCollector(
                user_agent=f"Atlas/{i}.0 (Memory Test)",
                timeout=10,
                max_concurrent=2
            )
            html_parser = HTMLParser()
            text_normalizer = TextNormalizer()

            components.append({
                'rss': rss_collector,
                'parser': html_parser,
                'normalizer': text_normalizer
            })

        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = peak_memory - initial_memory

        # æ€§èƒ½è¦æ±‚: å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡100MB
        assert memory_increase <= 100.0, f"å†…å­˜ä½¿ç”¨è¿‡å¤š: {memory_increase:.2f}MB"

        # æ¸…ç†ç»„ä»¶
        del components

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_recovered = peak_memory - final_memory

        print(f"âœ… å†…å­˜ä½¿ç”¨æ€§èƒ½è¾¾æ ‡:")
        print(f"   - åˆå§‹å†…å­˜: {initial_memory:.2f}MB")
        print(f"   - å³°å€¼å†…å­˜: {peak_memory:.2f}MB")
        print(f"   - å†…å­˜å¢é•¿: {memory_increase:.2f}MB")
        print(f"   - å†…å­˜å›æ”¶: {memory_recovered:.2f}MB")

    def test_perf_04_concurrent_processing(self, test_environment):
        """æ€§èƒ½æµ‹è¯•4: å¹¶å‘å¤„ç†èƒ½åŠ›"""
        print("ğŸš€ æ€§èƒ½æµ‹è¯• 4: å¹¶å‘å¤„ç†èƒ½åŠ›")

        html_parser = HTMLParser()
        text_normalizer = TextNormalizer()

        test_html = """
        <html>
        <body>
            <h1>å¹¶å‘æµ‹è¯•æ–‡æ¡£</h1>
            <p>è¿™æ˜¯ä¸€ä¸ªç”¨äºå¹¶å‘æ€§èƒ½æµ‹è¯•çš„æ–‡æ¡£ã€‚</p>
        </body>
        </html>
        """

        async def process_document(doc_id):
            """å¤„ç†å•ä¸ªæ–‡æ¡£çš„å¼‚æ­¥å‡½æ•°"""
            try:
                parsed = html_parser.parse_html_content(
                    test_html,
                    url=f"https://example.com/concurrent-{doc_id}",
                    title_selector="h1",
                    content_selector="p"
                )

                normalized = text_normalizer.normalize_text(parsed.content)

                return {
                    'doc_id': doc_id,
                    'success': True,
                    'title': parsed.title,
                    'content_length': len(normalized)
                }
            except Exception as e:
                return {
                    'doc_id': doc_id,
                    'success': False,
                    'error': str(e)
                }

        async def concurrent_test():
            num_documents = 100
            concurrency_levels = [5, 10, 20]

            for concurrency in concurrency_levels:
                print(f"   æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")

                start_time = time.time()

                # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
                semaphore = asyncio.Semaphore(concurrency)

                async def limited_process(doc_id):
                    async with semaphore:
                        return await process_document(doc_id)

                # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
                tasks = [limited_process(i) for i in range(num_documents)]
                results = await asyncio.gather(*tasks)

                end_time = time.time()
                total_time = end_time - start_time

                successful = sum(1 for r in results if r['success'])
                throughput = successful / total_time

                # æ€§èƒ½è¦æ±‚: å¹¶å‘å¤„ç†ååé‡ä¸åº”ä½äº10æ–‡æ¡£/ç§’
                assert throughput >= 10.0, f"å¹¶å‘çº§åˆ«{concurrency}: ååé‡è¿‡ä½ {throughput:.1f}æ–‡æ¡£/ç§’"

                # æ€§èƒ½è¦æ±‚: æˆåŠŸç‡ä¸åº”ä½äº95%
                success_rate = successful / num_documents
                assert success_rate >= 0.95, f"å¹¶å‘çº§åˆ«{concurrency}: æˆåŠŸç‡è¿‡ä½ {success_rate:.1%}"

                print(f"     - æ—¶é—´: {total_time:.2f}ç§’")
                print(f"     - æˆåŠŸ: {successful}/{num_documents}")
                print(f"     - ååé‡: {throughput:.1f}æ–‡æ¡£/ç§’")

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(concurrent_test())
        finally:
            loop.close()

        print("âœ… å¹¶å‘å¤„ç†æ€§èƒ½è¾¾æ ‡")

    def test_perf_05_system_stability(self, test_environment):
        """æ€§èƒ½æµ‹è¯•5: ç³»ç»Ÿç¨³å®šæ€§"""
        print("ğŸš€ æ€§èƒ½æµ‹è¯• 5: ç³»ç»Ÿç¨³å®šæ€§")

        rss_collector = RSSCollector(
            user_agent="Atlas/1.0 (Stability Test)",
            timeout=15,
            max_concurrent=3
        )

        html_parser = HTMLParser()
        text_normalizer = TextNormalizer()

        async def stability_test():
            # è¿è¡Œé•¿æ—¶é—´çš„æ··åˆæ“ä½œ
            num_iterations = 30
            errors = 0
            successful_operations = 0

            start_time = time.time()

            for iteration in range(num_iterations):
                try:
                    # äº¤æ›¿æ‰§è¡Œä¸åŒæ“ä½œ
                    if iteration % 3 == 0:
                        # RSSé‡‡é›†æ“ä½œ
                        result = await rss_collector.collect_rss(
                            "https://feeds.bbci.co.uk/news/rss.xml"
                        )
                        if result and len(result.items) > 0:
                            successful_operations += 1
                        else:
                            errors += 1

                    elif iteration % 3 == 1:
                        # HTMLå¤„ç†æ“ä½œ
                        parsed = html_parser.parse_html_content(
                            "<html><body><h1>Stability Test</h1></body></html>",
                            url="https://example.com/stability",
                            title_selector="h1"
                        )
                        if parsed and parsed.title:
                            successful_operations += 1
                        else:
                            errors += 1

                    else:
                        # æ–‡æœ¬æ ‡å‡†åŒ–æ“ä½œ
                        normalized = text_normalizer.normalize_text(
                            "è¿™æ˜¯ä¸€æ®µç”¨äºç¨³å®šæ€§æµ‹è¯•çš„æ–‡æœ¬å†…å®¹ã€‚"
                        )
                        if len(normalized.strip()) > 0:
                            successful_operations += 1
                        else:
                            errors += 1

                    # æ¯10æ¬¡è¿­ä»£è¾“å‡ºè¿›åº¦
                    if (iteration + 1) % 10 == 0:
                        print(f"   è¿›åº¦: {iteration + 1}/{num_iterations}")

                except Exception as e:
                    errors += 1
                    print(f"   è¿­ä»£ {iteration} é”™è¯¯: {e}")

            end_time = time.time()
            total_time = end_time - start_time

            # ç¨³å®šæ€§è¦æ±‚: é”™è¯¯ç‡ä¸åº”è¶…è¿‡10%
            error_rate = errors / num_iterations
            assert error_rate <= 0.10, f"é”™è¯¯ç‡è¿‡é«˜: {error_rate:.1%}"

            # ç¨³å®šæ€§è¦æ±‚: æ€»ä½“æˆåŠŸç‡ä¸åº”ä½äº90%
            success_rate = successful_operations / num_iterations
            assert success_rate >= 0.90, f"æˆåŠŸç‡è¿‡ä½: {success_rate:.1%}"

            print(f"âœ… ç³»ç»Ÿç¨³å®šæ€§è¾¾æ ‡:")
            print(f"   - æ€»è¿­ä»£: {num_iterations}")
            print(f"   - æˆåŠŸæ“ä½œ: {successful_operations}")
            print(f"   - é”™è¯¯æ•°: {errors}")
            print(f"   - æˆåŠŸç‡: {success_rate:.1%}")
            print(f"   - æ€»æ—¶é—´: {total_time:.2f}ç§’")

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(stability_test())
        finally:
            loop.close()


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹Atlasç³»ç»Ÿæ€§èƒ½éªŒæ”¶æµ‹è¯•")
    print("=" * 50)

    pytest.main([__file__, "-v", "-s"])